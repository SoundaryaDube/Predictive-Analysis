# -*- coding: utf-8 -*-
"""HealthHack_Diabetic_Retinopathy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kxJDEZdQ00qOjvtxNYpvWAAjX9GX7Bka
"""

import sys

if "google.colab" in sys.modules:
    from google.colab import drive
    drive.mount('/content/drive')
else:
    print("Not running in Google Colab. Skipping drive.mount().")


import pandas as pd
import zipfile
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Input
import os

# Path to the ZIP file
zip_path = "/content/drive/MyDrive/B. Disease Grading.zip"

# Open the ZIP file and list its contents
with zipfile.ZipFile(zip_path, 'r') as z:
    print("Files inside ZIP:", z.namelist())  # List all files inside ZIP

with zipfile.ZipFile(zip_path, 'r') as z:
    file_name = z.namelist()[0]  # Change index if needed
    with z.open(file_name) as f:
        content = f.read()
        print("File content preview:", content[:500])  # Preview first 500 bytes

with zipfile.ZipFile(zip_path, 'r') as z:
    file_list = z.namelist()
    print("Files inside ZIP:", file_list)

    # Check file sizes
    for file in file_list:
        with z.open(file) as f:
            content = f.read()
            print(f"{file} - Size: {len(content)} bytes")

import subprocess

subprocess.run(["unzip", "/content/drive/MyDrive/B. Disease Grading.zip", "-d", "/content/extracted"])

# Define image size & batch size
IMG_SIZE = (224, 224)
BATCH_SIZE = 32

# Define dataset path
train_dir = '/content/extracted/B. Disease Grading/1. Original Images/a. Training Set'
valid_dir = '/content/extracted/B. Disease Grading/1. Original Images/b. Testing Set'  # If validation images exist

print(os.listdir(train_dir))

# Define the model architecture
base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False

inputs = Input(shape=(224, 224, 3))
x = base_model(inputs)
x = Flatten()(x)

# Model Architecture
model = Sequential([
    base_model,
    Flatten(),
    Dense(128, activation='relu'),
    Dense(5, activation='softmax')  # 5 classes for disease grading
])

# Compile the model
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

os.listdir('/content/extracted/B. Disease Grading/1. Original Images/a. Training Set')  # Check contents of the training folder
os.listdir('/content/extracted/B. Disease Grading/1. Original Images/b. Testing Set')  # Check contents of the validation folder

# List the files in the training directory
train_files = os.listdir('/content/extracted/B. Disease Grading/1. Original Images/a. Training Set')
valid_files = os.listdir('/content/extracted/B. Disease Grading/1. Original Images/b. Testing Set')

print("Training files:", train_files)
print("Validation files:", valid_files)

import os
import pandas as pd
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np

# Define image paths and labels
train_dir = '/content/extracted/B. Disease Grading/1. Original Images/a. Training Set'
valid_dir = '/content/extracted/B. Disease Grading/1. Original Images/b. Testing Set'

train_files = os.listdir(train_dir)
valid_files = os.listdir(valid_dir)

# list of image paths (use actual paths and labels)
train_image_paths = [os.path.join(train_dir, file) for file in train_files]
valid_image_paths = [os.path.join(valid_dir, file) for file in valid_files]

# Assuming labels are predefined or extracted (e.g., 0, 1, 2 for classes)
train_labels = [np.random.randint(0, 5) for _ in range(len(train_image_paths))]
valid_labels = [np.random.randint(0, 5) for _ in range(len(valid_image_paths))]

df_train = pd.DataFrame({'file': train_image_paths, 'label': [str(label) for label in train_labels]})
df_valid = pd.DataFrame({'file': valid_image_paths, 'label': [str(label) for label in valid_labels]})

import tensorflow as tf
import os

# Paths to the dataset
train_dir = '/content/extracted/B. Disease Grading/1. Original Images/a. Training Set'
valid_dir = '/content/extracted/B. Disease Grading/1. Original Images/b. Testing Set'

def load_image(file_path):
    image = tf.io.read_file(file_path)
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.image.resize(image, [224, 224])
    image = image / 255.0  # Normalize
    return image

def create_dataset(data_dir):
    file_paths = [os.path.join(data_dir, fname) for fname in os.listdir(data_dir)]
    train_labels = [np.random.randint(0, 5) for _ in range(len(train_image_paths))]
    valid_labels = [np.random.randint(0, 5) for _ in range(len(valid_image_paths))]
    path_ds = tf.data.Dataset.from_tensor_slices(file_paths)
    image_ds = path_ds.map(load_image)
    train_labels_ds = tf.data.Dataset.from_tensor_slices(train_labels)
    valid_labels_ds = tf.data.Dataset.from_tensor_slices(valid_labels)
    return tf.data.Dataset.zip((image_ds, train_labels_ds, valid_labels_ds))

train_data = create_dataset(train_dir)
valid_data = create_dataset(valid_dir)

# Set batch size
train_data = train_data.batch(32)
valid_data = valid_data.batch(32)

from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, horizontal_flip=True)
valid_datagen = ImageDataGenerator(rescale=1./255)
# Ensure categorical labels are correctly handled by `flow_from_dataframe`
train_data = train_datagen.flow_from_dataframe(
    dataframe=df_train,
    directory=None,
    x_col='file',
    y_col='label',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'  # Automatically handles one-hot encoding
)

valid_data = valid_datagen.flow_from_dataframe(
    dataframe=df_valid,
    directory=None,
    x_col='file',
    y_col='label',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'  # Automatically handles one-hot encoding
)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Define the model
model = Sequential([
    EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3)),
    Flatten(),
    Dropout(0.5),  # Dropout to reduce overfitting
    Dense(128, activation='relu'),
    Dropout(0.5),  # Another Dropout layer
    Dense(5, activation='softmax')  # Assuming 5 classes for disease grading
])

# Compile the model
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

# Early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model with early stopping
model.fit(train_data, epochs=10, validation_data=valid_data,
          steps_per_epoch=len(train_data), validation_steps=len(valid_data),
          callbacks=[early_stopping])

# Evaluate model on the validation dataset
val_loss, val_accuracy = model.evaluate(valid_data)
print(f"Validation Loss: {val_loss}")
print(f"Validation Accuracy: {val_accuracy}")

# Save Model
model.save("diabetic_retinopathy_model.keras")

history = model.fit(train_data, epochs=10, validation_data=valid_data,
                    steps_per_epoch=len(train_data), validation_steps=len(valid_data))

import matplotlib.pyplot as plt
import seaborn as sns

# Example: Visualize training and validation accuracy
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

import tensorflow as tf

# Load and preprocess the image
image_path = '/content/extracted/B. Disease Grading/1. Original Images/b. Testing Set/IDRiD_086.jpg'  # Assuming the image is in the current directory
image = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))  # Resize to match model input
image = tf.keras.preprocessing.image.img_to_array(image)
image = tf.expand_dims(image, axis=0)  # Add batch dimension

# Make predictions
predictions = model.predict(image)

# Output the raw predictions (class probabilities)
print("Predictions (raw probabilities):", predictions)

# If your model is for classification, get the predicted class index
predicted_class_index = np.argmax(predictions, axis=1).item()  # Get the predicted class index

# Get the class labels from the training data
class_labels = list(valid_data.class_indices.keys())  # Use the class labels from the validation data generator

# Map the predicted class index to the class label
predicted_class_label = class_labels[predicted_class_index]

# Display the predicted class label
print(f"Predicted Class Index: {predicted_class_index}")
print(f"Predicted Class Label: {predicted_class_label}")

from tensorflow.keras.models import load_model

model = load_model("diabetic_retinopathy_model.keras")

from sklearn.metrics import confusion_matrix

# Get model predictions
valid_data.reset()  # Make sure to reset the generator for correct batch iteration
predictions = model.predict(valid_data, verbose=1)
predicted_classes = np.argmax(predictions, axis=1)

# Get the true class labels
true_classes = valid_data.classes

# Compute confusion matrix
cm = confusion_matrix(true_classes, predicted_classes)

# Plot confusion matrix using seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=valid_data.class_indices.keys(), yticklabels=valid_data.class_indices.keys())
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

import subprocess

subprocess.run(["pip", "install", "streamlit", "tensorflow", "pillow", "boto3"])


import tensorflow as tf

# Load the trained model
model = tf.keras.models.load_model("diabetic_retinopathy_model.keras")

import streamlit as st
import tensorflow as tf
import numpy as np
from PIL import Image

# Load the trained model
model = tf.keras.models.load_model("diabetic_retinopathy_model.keras")

# Define the function to preprocess the image
def preprocess_image(image):
    image = image.resize((224, 224))  # Resize image
    image = np.array(image) / 255.0  # Normalize pixel values
    image = np.expand_dims(image, axis=0)  # Add batch dimension
    return image

# Streamlit UI
st.title("Diabetic Retinopathy Detection")
st.write("Upload a retinal image to classify the severity of diabetic retinopathy.")

# Upload Image
uploaded_file = st.file_uploader("Upload an image...", type=["jpg", "png", "jpeg"])

if uploaded_file is not None:
    # Display uploaded image
    image = Image.open(uploaded_file)
    st.image(image, caption="Uploaded Image", use_column_width=True)

    # Preprocess & Predict
    processed_image = preprocess_image(image)
    prediction = model.predict(processed_image)
    predicted_class = np.argmax(prediction)

    # Define class labels
    class_labels = ["No DR", "Mild", "Moderate", "Severe", "Proliferative DR"]

    # Show result
    st.write(f"### Prediction: {class_labels[predicted_class]}")

